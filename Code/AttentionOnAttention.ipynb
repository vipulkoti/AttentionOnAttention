{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3389d21c",
   "metadata": {},
   "source": [
    "<h1>Self-Attention</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555d7f5",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism that calculates attention weights by analyzing the relationships between different parts of a single input sequence, such as words in a sentence or pixels in an image. It learns how these elements relate to each other within the same input.\n",
    "\n",
    "Traditional Sequential models, like RNNs, process data sequentially and depend on the context built so far. These models have trouble maintaining context over long sequences due to issues like the exploding or vanishing gradient problem and lack of parallel processing capabilities. Another important thing to understand is that the importance of previous words generally doesn't depend on the order in which they appear.\n",
    "\n",
    "Let's understand this and the need for self-attention from an example:\n",
    "\n",
    "Consider the sentence: \"The chef prepared a delicious meal, and it was served with wine.\" This sentence has 12 words, or tokens. If we focus on the word \"it,\" we need to understand what \"it\" refers to in the context of the sentence. The words \"was\" and \"served\" are close to \"it\" in terms of proximity, but they don't help us understand the meaning of \"it.\" Instead, the word \"meal,\" which appears earlier in the sentence, is much more relevant because \"it\" refers to the \"meal.\"\n",
    "\n",
    "In this example, proximity isn't the key factor for understanding; the context provided by the word \"meal\" is what clarifies the meaning of \"it.\"\n",
    "\n",
    "When a computer processes this sentence, each word is represented as a token with a word embedding, which is a numerical vector representing the word's meaning. In a word embedding space, words with similar meanings or that are used in similar contexts have embeddings that are close to each other. For instance, the word \"chef\" might be close to \"cook\" and \"kitchen,\" while \"wine\" might be close to \"beverage\" and \"drink.\" To learn more about word embedding, visit [this link](#).\n",
    "\n",
    "Initially, these embeddings don't capture the relationships between words like \"it\" and \"wine\" in our sentence; they lack context for our sentences. The goal of self-attention is to refine these general embeddings with the current context so that they include more context about the current sentence as a whole.\n",
    "\n",
    "Using self-attention, we calculate how much each word in the sentence should \"attend to\" or focus on every other word to understand its context better. For the word \"it\" in our example, the attention mechanism would give more weight to \"meal\" than to \"was\" or \"served\" because \"meal\" provides the necessary context to understand what \"it\" refers to. Generally, we can say that by using self-attention, we effectively enhance the word embeddings so that they carry more contextual information, enabling the model to understand the sentence more accurately.\n",
    "\n",
    "## Level 1 - Basic Self-Attention Calculations\n",
    "\n",
    "We will understand the basic self-attention calculations without weights (we will add weights in the next section).\n",
    "\n",
    "Think of the above sentence \"The chef prepared a delicious meal, and it was served with wine.\" First, we calculate vocabulary and turn words into tokens \\(\\{t1, t2, t3, \\ldots, tn\\}\\). These tokens are then converted into word embeddings \\(\\{e1, e2, e3, \\ldots, en\\}\\). (Covered here in word embedding discussion).\n",
    "\n",
    "Now these word embeddings are used pairwise (e.g., \\(e2\\) with all \\(e1, e2, e3, \\ldots, en\\)) to get relevant attention scores \\(\\{a1, a2, a3, \\ldots, an\\}\\) (how important all the other words are for word \\(e2\\)). For the word \"it,\" the word \"meal\" will have the highest attention score. These scores are calculated using the dot product of word embeddings (we are not considering weights in this section).\n",
    "\n",
    "Then the attention scores are converted to attention weights \\(\\{w1, w2, w3, \\ldots, wn\\}\\) by normalizing the attention scores. In the last step, using these weights, a new representation for each word is generated as the weighted sum of all the other words. (Note weight for each word is defined by the attention/relevance scores).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15eeaa9",
   "metadata": {},
   "source": [
    "### Steps for Self-Attention\n",
    "\n",
    "1. **Tokenization & Embedding**:  \n",
    "   Represent each word as a vector (tokens: `t1, t2, t3,... tn`, embeddings: `e1, e2, e3,... en`).\n",
    "\n",
    "2. **Pairwise Comparison**:  \n",
    "   Compute attention scores between word pairs (e.g., focus of \"meal\" when interpreting \"it\").\n",
    "\n",
    "3. **Normalization**:  \n",
    "   Apply softmax to convert attention scores to attention weights.\n",
    "\n",
    "4. **Contextual Representation**:  \n",
    "   Create a new context-aware word vector by using weighted sums of all embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a41cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Self-Attention (Without Weights) ###\n",
    "import torch\n",
    "#Step 1\n",
    "word_embeddings = torch.tensor(\n",
    "  [[0.32, 0.68, 0.45], # The      (x^1)\n",
    "   [0.71, 0.23, 0.89], # chef     (x^2)\n",
    "   [0.55, 0.92, 0.37], # prepared (x^3)\n",
    "   [0.18, 0.79, 0.60], # a        (x^4)\n",
    "   [0.84, 0.41, 0.13], # delicious(x^5)\n",
    "   [0.29, 0.63, 0.76], # meal     (x^6)\n",
    "   [0.50, 0.15, 0.95], # and      (x^7)\n",
    "   [0.67, 0.38, 0.82], # it       (x^8)\n",
    "   [0.43, 0.91, 0.26], # was      (x^9)\n",
    "   [0.75, 0.20, 0.58], # served   (x^10)\n",
    "   [0.36, 0.72, 0.49], # with     (x^11)\n",
    "   [0.88, 0.54, 0.11]] # wine     (x^12)\n",
    ")\n",
    "\n",
    "# Step 2 Calculate attention scores (Pairwise compare)\n",
    "attn_scores = word_embeddings @ word_embeddings.T\n",
    "\n",
    "# Step 3 Calculate attention weights (Normalize)\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "# Step 4 Calculate context vectors\n",
    "all_context_vecs = attn_weights @ word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d55c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5270, 0.5664, 0.5374],\n",
      "        [0.5533, 0.5059, 0.5825],\n",
      "        [0.5316, 0.5783, 0.5197],\n",
      "        [0.5150, 0.5726, 0.5456],\n",
      "        [0.5655, 0.5434, 0.5146],\n",
      "        [0.5233, 0.5521, 0.5616],\n",
      "        [0.5458, 0.5044, 0.5926],\n",
      "        [0.5477, 0.5204, 0.5716],\n",
      "        [0.5280, 0.5851, 0.5142],\n",
      "        [0.5601, 0.5151, 0.5592],\n",
      "        [0.5271, 0.5664, 0.5382],\n",
      "        [0.5638, 0.5516, 0.5077]])\n"
     ]
    }
   ],
   "source": [
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9db52e",
   "metadata": {},
   "source": [
    "<h2>Add Weights to self-Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc2844",
   "metadata": {},
   "source": [
    "### Trainable Weights in Self-Attention\n",
    "\n",
    "In self-attention, three weight matrices are used to focus on different aspects of the input:\n",
    "\n",
    "1. **Query (Q) Matrix**:  \n",
    "   Helps determine what the model should search for when analyzing each word.  \n",
    "   (Q: What do you want?)\n",
    "\n",
    "2. **Key (K) Matrix**:  \n",
    "   Identifies which information within each word is relevant to the query.  \n",
    "   (Q: Who do you get it from?)\n",
    "\n",
    "3. **Value (V) Matrix**:  \n",
    "   Contains the actual content or meaning of the words.  \n",
    "   (Q: What do you get?)\n",
    "   \n",
    "Together, they help the model determine how much attention to give each word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b441aa6",
   "metadata": {},
   "source": [
    "### Steps for Self-Attention (with Weight Matrices)\n",
    "\n",
    "1. **Generate Query, Key, Value Vectors**:  \n",
    "   Multiply input embeddings with Q, K, V weight matrices to get Query, Key, Value vectors.\n",
    "\n",
    "2. **Compute Attention Scores**:  \n",
    "   Matrix multiply Query and Key vectors to get raw attention score matrix.\n",
    "\n",
    "3. **Normalize Scores**:  \n",
    "   Apply softmax to raw attention scores to get attention weights.\n",
    "\n",
    "4. **Generate Context Vector**:  \n",
    "   Matrix multiply attention weights with the Value matrix to generate a context vector for each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f47312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1348, 0.1801],\n",
      "        [0.1358, 0.1782],\n",
      "        [0.1361, 0.1776],\n",
      "        [0.1346, 0.1803],\n",
      "        [0.1358, 0.1782],\n",
      "        [0.1349, 0.1798],\n",
      "        [0.1348, 0.1799],\n",
      "        [0.1359, 0.1780],\n",
      "        [0.1355, 0.1788],\n",
      "        [0.1355, 0.1787],\n",
      "        [0.1351, 0.1796],\n",
      "        [0.1362, 0.1774]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Self-Attention (with weights) ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1240)\n",
    "\n",
    "# Define input and output dimensions\n",
    "d_in = word_embeddings.shape[1]  # Input embedding size (3 in this case)\n",
    "d_out = 2  # Output embedding size\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, use_bias=False):\n",
    "        super().__init__()\n",
    "        # Linear transformations for Query, Key, and Value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=use_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=use_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1 Transform input to Query, Key, and Value\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # Step 2 Calculate attention scores\n",
    "        attn_scores = queries @ keys.T\n",
    "        \n",
    "        # Step 3 Apply scaling factor and softmax to get attention weights\n",
    "        scaling_factor = keys.shape[-1] ** 0.5\n",
    "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
    "\n",
    "        # Step 4 Compute context vectors\n",
    "        context_vectors = attn_weights @ values\n",
    "        return context_vectors\n",
    "\n",
    "# Initialize the SelfAttention module\n",
    "torch.manual_seed(1240)  # Reset seed for consistent results\n",
    "self_attention = SelfAttention(d_in, d_out)\n",
    "\n",
    "# Apply self-attention to word embeddings\n",
    "result = self_attention(word_embeddings)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527b9c9",
   "metadata": {},
   "source": [
    "<h2>Causal Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470557d7",
   "metadata": {},
   "source": [
    "### Causal Attention (Masked Attention)\n",
    "\n",
    "In some tasks, like next-word prediction, the model needs to be **causal**, meaning it cannot use future information to predict the current word. For instance, an Auto Complete model can only see past words when predicting the next one.\n",
    "\n",
    "Causal Attention (or Masked Attention) achieves this by using a **causal mask** that hides future words during the process. This mask sets the attention scores for future words to zero before the model applies attention weights.\n",
    "\n",
    "### Summarizing Steps for Causal Attention\n",
    "\n",
    "1. **Compute Attention Scores**:  \n",
    "   Calculate attention scores between word positions, as done in self-attention.\n",
    "\n",
    "2. **Apply Causal Mask**:  \n",
    "   Mask out the scores for future words by setting them to negative infinity (-∞) to ensure they are ignored. This is done by masking scores above the diagonal in the attention matrix.\n",
    "\n",
    "3. **Normalize and Apply Attention**:  \n",
    "   After applying the causal mask, normalize the scores using softmax. Future words will have zero attention weight because their scores were set to negative infinity. The resulting attention weights are then used by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70a5e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 3])\n",
      "tensor([[[0.0872, 0.2233],\n",
      "         [0.1996, 0.0526],\n",
      "         [0.1382, 0.1952],\n",
      "         [0.1382, 0.1830],\n",
      "         [0.1097, 0.2292],\n",
      "         [0.1286, 0.1924],\n",
      "         [0.1608, 0.1259],\n",
      "         [0.1748, 0.1068],\n",
      "         [0.1528, 0.1494],\n",
      "         [0.1562, 0.1402],\n",
      "         [0.1502, 0.1500],\n",
      "         [0.1362, 0.1774]],\n",
      "\n",
      "        [[0.0872, 0.2233],\n",
      "         [0.1996, 0.0526],\n",
      "         [0.1382, 0.1952],\n",
      "         [0.1382, 0.1830],\n",
      "         [0.1097, 0.2292],\n",
      "         [0.1286, 0.1924],\n",
      "         [0.1608, 0.1259],\n",
      "         [0.1748, 0.1068],\n",
      "         [0.1528, 0.1494],\n",
      "         [0.1562, 0.1402],\n",
      "         [0.1502, 0.1500],\n",
      "         [0.1362, 0.1774]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vectors.shape: torch.Size([2, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "### Caual Attention with causal and dropout mask on batched input###\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a batch by stacking the word embeddings twice\n",
    "batch = torch.stack((word_embeddings, word_embeddings), dim=0)\n",
    "print(batch.shape)  # Shape: (2, 12, 3) - 2 inputs, 12 tokens each, 3-dimensional embeddings\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        # Linear transformations for Query, Key, and Value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create an upper triangular mask to ensure causality\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # b: batch size, num_tokens: sequence length, d_in: input dimension\n",
    "        \n",
    "        # Transform input to Query, Key, and Value\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        \n",
    "        # Apply causal mask to prevent attending to future tokens\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], \n",
    "            -torch.inf\n",
    "        )\n",
    "        \n",
    "        # Apply scaling factor and softmax to get attention weights\n",
    "        scaling_factor = keys.shape[-1] ** 0.5\n",
    "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute context vectors\n",
    "        context_vectors = attn_weights @ values\n",
    "        return context_vectors\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1240)\n",
    "\n",
    "# Initialize CausalAttention module\n",
    "context_length = batch.shape[1]\n",
    "d_in = batch.shape[2]\n",
    "d_out = 2  # Output embedding size\n",
    "causal_attention = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "\n",
    "# Apply causal attention to the batch\n",
    "context_vectors = causal_attention(batch)\n",
    "\n",
    "print(context_vectors)\n",
    "print(\"context_vectors.shape:\", context_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961444d",
   "metadata": {},
   "source": [
    "<h2> Multi-Head Attention </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70688e4d",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The core concept of multi-head attention is to apply the attention mechanism several times in parallel, each with its own set of learned linear transformations. This approach enables the model to simultaneously focus on information from different representation subspaces and at various positions within the input, capturing a richer set of dependencies and patterns.  \n",
    "[Source- Attention is all you need. NeurIPS, 2017]\n",
    "\n",
    "Let's go back to our old example, \"The chef prepared a delicious meal, and it was served with wine.\" If we focus on the word \"meal,\" we can see that there are several other words in the sentence that have significant relevance to it. Specifically, the words \"chef,\" \"prepared,\" \"delicious,\" and \"wine\" all provide important context that enhances our understanding of the word \"meal.\"\n",
    "\n",
    "- The word **\"chef\"** tells us who is responsible for making the meal.\n",
    "- The word **\"prepared\"** indicates the action taken to create the meal.\n",
    "- The word **\"delicious\"** describes the quality of the meal.\n",
    "- The phrase **\"with wine\"** suggests what accompanies the meal when served.\n",
    "\n",
    "In a simple attention mechanism, a single attention head might struggle to simultaneously capture all these nuances and associations. This could lead to missing some relevant words or not fully capturing the relationships between \"meal\" and other words. However, with multi-head attention, we can allocate different attention heads to focus on different aspects of the sentence. For instance, one head might focus on the relationship between \"meal\" and \"chef,\" another on \"meal\" and \"delicious,\" and yet another on \"meal\" and \"wine.\" By distributing the attention across multiple heads, the model can capture a richer and more nuanced understanding of the word \"meal\" in context. This richer understanding of all relevant words and relationships enhances the model's ability.\n",
    "\n",
    "### Summarizing Steps for Multi-Head Attention\n",
    "\n",
    "1. **Learn Different Relationships**:  \n",
    "   Run multiple heads of attention mechanism on the input. Each head will use different weight matrices and generate different attention weights.\n",
    "\n",
    "2. **Combine Attention Scores**:  \n",
    "   All the attention scores from the multiple heads are concatenated and then combined using linear layer projection.\n",
    "\n",
    "3. **Generate Context Vector**:  \n",
    "   The combined attention score from step 2 is used to generate the final enriched representation of the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "273c7953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6238, -0.3816],\n",
      "         [ 0.4784, -0.3510],\n",
      "         [ 0.5800, -0.3691],\n",
      "         [ 0.5747, -0.3686],\n",
      "         [ 0.6151, -0.3764],\n",
      "         [ 0.5870, -0.3709],\n",
      "         [ 0.5370, -0.3618],\n",
      "         [ 0.5201, -0.3580],\n",
      "         [ 0.5527, -0.3642],\n",
      "         [ 0.5459, -0.3633],\n",
      "         [ 0.5536, -0.3648],\n",
      "         [ 0.5761, -0.3689]],\n",
      "\n",
      "        [[ 0.6238, -0.3816],\n",
      "         [ 0.4784, -0.3510],\n",
      "         [ 0.5800, -0.3691],\n",
      "         [ 0.5747, -0.3686],\n",
      "         [ 0.6151, -0.3764],\n",
      "         [ 0.5870, -0.3709],\n",
      "         [ 0.5370, -0.3618],\n",
      "         [ 0.5201, -0.3580],\n",
      "         [ 0.5527, -0.3642],\n",
      "         [ 0.5459, -0.3633],\n",
      "         [ 0.5536, -0.3648],\n",
      "         [ 0.5761, -0.3689]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "### Multi-Head Attention ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension of each attention head\n",
    "\n",
    "        # Linear projections for Query, Key, and Value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask to prevent attending to future tokens\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # b: batch size, num_tokens: sequence length\n",
    "\n",
    "        # Linear projections\n",
    "        keys = self.W_key(x)      # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for attention computation\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Apply causal mask\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        context_vec = attn_weights @ values  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        \n",
    "        # Reshape and combine heads\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous()  # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.view(b, num_tokens, self.d_out)  # (b, num_tokens, d_out)\n",
    "        \n",
    "        # Final output projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1240)\n",
    "\n",
    "# Get dimensions from the batch\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2  # Output dimension\n",
    "\n",
    "# Initialize MultiHeadAttention\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n",
    "\n",
    "# Apply multi-head attention to the batch\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baed9e3",
   "metadata": {},
   "source": [
    "## Cross-Attention Mechanism\n",
    "\n",
    "Cross-attention allows a model to focus on relevant parts of one sequence while processing another. When we compare it to self-attention, where a model weighs the importance of different parts of the same input sequence (i.e., token to token in a sentence), cross-attention relates to different inputs. The two sequences could belong to different modalities or represent different sets of information.\n",
    "\n",
    "## Example\n",
    "\n",
    "For our example, we will break our original sentence into two sequences and compute cross-attention between those two sequences. \n",
    "\n",
    "We split the sentence into two parts:\n",
    "\n",
    "- \\( x_1 \\): \"The chef prepared a delicious meal\" (first 6 words)\n",
    "- \\( x_2 \\): \"and it was served with wine\" (last 6 words)\n",
    "\n",
    "The cross-attention mechanism will allow the first part of the sentence to attend to the second part. This means each word in the first part will compute attention weights for each word in the second part.\n",
    "\n",
    "### Illustration\n",
    "\n",
    "In a cross-attention scenario, each word in \\( x_1 \\) will focus on and compute relevance scores with every word in \\( x_2 \\). This allows the model to incorporate information from \\( x_2 \\) into the processing of \\( x_1 \\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6750a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-attention result shape: torch.Size([6, 2])\n",
      "Cross-attention result:\n",
      " tensor([[0.5326, 0.2634],\n",
      "        [0.5321, 0.2654],\n",
      "        [0.5345, 0.2637],\n",
      "        [0.5325, 0.2636],\n",
      "        [0.5334, 0.2634],\n",
      "        [0.5322, 0.2642]], grad_fn=<MmBackward0>)\n",
      "The: [0.5325765609741211, 0.26338499784469604]\n",
      "chef: [0.5320825576782227, 0.2653651535511017]\n",
      "prepared: [0.534509539604187, 0.2637292146682739]\n",
      "a: [0.5324926376342773, 0.2635837197303772]\n",
      "delicious: [0.533425509929657, 0.2634294927120209]\n",
      "meal: [0.532193660736084, 0.26424530148506165]\n"
     ]
    }
   ],
   "source": [
    "### Cross Attention ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "word_embeddings = torch.tensor(\n",
    "  [[0.32, 0.68, 0.45], # The      (x^1)\n",
    "   [0.71, 0.23, 0.89], # chef     (x^2)\n",
    "   [0.55, 0.92, 0.37], # prepared (x^3)\n",
    "   [0.18, 0.79, 0.60], # a        (x^4)\n",
    "   [0.84, 0.41, 0.13], # delicious(x^5)\n",
    "   [0.29, 0.63, 0.76], # meal     (x^6)\n",
    "   [0.50, 0.15, 0.95], # and      (x^7)\n",
    "   [0.67, 0.38, 0.82], # it       (x^8)\n",
    "   [0.43, 0.91, 0.26], # was      (x^9)\n",
    "   [0.75, 0.20, 0.58], # served   (x^10)\n",
    "   [0.36, 0.72, 0.49], # with     (x^11)\n",
    "   [0.88, 0.54, 0.11]] # wine     (x^12)\n",
    ")\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        \n",
    "        # Learnable weight matrices for Query, Key, and Value\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        # Compute queries from x_1\n",
    "        queries = x_1 @ self.W_query\n",
    "        \n",
    "        # Compute keys and values from x_2\n",
    "        keys = x_2 @ self.W_key\n",
    "        values = x_2 @ self.W_value\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = queries @ keys.T\n",
    "        \n",
    "        # Apply scaling factor and softmax to get attention weights\n",
    "        scaling_factor = self.d_out_kq ** 0.5\n",
    "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
    "        \n",
    "        # Compute context vectors\n",
    "        context_vectors = attn_weights @ values\n",
    "        \n",
    "        return context_vectors\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define dimensions\n",
    "d_in = word_embeddings.shape[1]  # Input dimension (3 in this case)\n",
    "d_out_kq = 4  # Output dimension for queries and keys\n",
    "d_out_v = 2   # Output dimension for values\n",
    "\n",
    "# Initialize CrossAttention module\n",
    "cross_attention = CrossAttention(d_in, d_out_kq, d_out_v)\n",
    "\n",
    "# Split the sentence into two parts for demonstration\n",
    "x_1 = word_embeddings[:6]  # \"The chef prepared a delicious meal\"\n",
    "x_2 = word_embeddings[6:]  # \"and it was served with wine\"\n",
    "\n",
    "# Apply cross-attention\n",
    "result = cross_attention(x_1, x_2)\n",
    "\n",
    "print(\"Cross-attention result shape:\", result.shape)\n",
    "print(\"Cross-attention result:\\n\", result)\n",
    "\n",
    "# Interpret the results\n",
    "for i, word in enumerate([\"The\", \"chef\", \"prepared\", \"a\", \"delicious\", \"meal\"]):\n",
    "    print(f\"{word}: {result[i].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1333237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-attention result shape: torch.Size([5, 2])\n",
      "Cross-attention result:\n",
      " tensor([[0.4803, 1.2236],\n",
      "        [0.4700, 1.2026],\n",
      "        [0.4785, 1.2194],\n",
      "        [0.4565, 1.1745],\n",
      "        [0.4840, 1.2308]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Cross Attention ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        \n",
    "        # Learnable weight matrices for Query, Key, and Value\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        # x_1: first input sequence (for queries)\n",
    "        # x_2: second input sequence (for keys and values)\n",
    "        \n",
    "        # Compute queries from x_1\n",
    "        queries = x_1 @ self.W_query  # Shape: (seq_len_1, d_out_kq)\n",
    "        \n",
    "        # Compute keys and values from x_2\n",
    "        keys = x_2 @ self.W_key       # Shape: (seq_len_2, d_out_kq)\n",
    "        values = x_2 @ self.W_value   # Shape: (seq_len_2, d_out_v)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = queries @ keys.T  # Shape: (seq_len_1, seq_len_2)\n",
    "        \n",
    "        # Apply scaling factor and softmax to get attention weights\n",
    "        scaling_factor = self.d_out_kq ** 0.5\n",
    "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
    "        \n",
    "        # Compute context vectors\n",
    "        context_vectors = attn_weights @ values  # Shape: (seq_len_1, d_out_v)\n",
    "        \n",
    "        return context_vectors\n",
    "\n",
    "# Example usage:\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Define dimensions\n",
    "d_in = 3       # Input dimension\n",
    "d_out_kq = 4   # Output dimension for queries and keys\n",
    "d_out_v = 2    # Output dimension for values\n",
    "\n",
    "# Create random input sequences\n",
    "seq_len_1 = 5\n",
    "seq_len_2 = 6\n",
    "x_1 = torch.rand(seq_len_1, d_in)\n",
    "x_2 = torch.rand(seq_len_2, d_in)\n",
    "\n",
    "# Initialize CrossAttention module\n",
    "cross_attention = CrossAttention(d_in, d_out_kq, d_out_v)\n",
    "\n",
    "# Apply cross-attention\n",
    "result = cross_attention(x_1, x_2)\n",
    "\n",
    "print(\"Cross-attention result shape:\", result.shape)\n",
    "print(\"Cross-attention result:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92964fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
