# Attention on Attention

## Analogy and Definition

Let's start with an analogy of the attention mechanism to human reading using "The Lord of the Rings."

Imagine you're reading "The Lord of the Rings," a complex story with a rich narrative and many characters. As you read, your brain naturally prioritizes certain details over others depending on the context. You don't give equal attention to every word or sentence. Instead, you:

- **Focus on Key Characters and Events**: For example, when reading about Frodo's journey to destroy the One Ring, you might pay close attention to his interactions with Gollum because you know Gollum's behavior is crucial for Frodo's quest.
- **Recall Important Details**: If Frodo mentions something about Rivendell, you might recall the significance of Rivendell from earlier chapters and use that context to understand the current situation better.
- **Adjust Focus Based on Context**: When the narrative shifts to a different storyline, like Aragorn's path to becoming king, you switch focus and remember the details relevant to Aragorn, such as his lineage and destiny.

When we read a sentence in a book, we use the context built and saved from previous sentences in the short-term or working memory. The context helps us understand the meaning of the current sentence. This selective focus is similar to how the attention mechanism works in deep learning models. The attention mechanism is analogous to short-term memory for transformers.

**Definition**: An attention mechanism in machine learning allows models to dynamically focus on the most relevant parts of the input data while processing the data, thereby enhancing the model's efficiency and accuracy in processing the data.

## What We Will Learn Next

- **Level 1 & Level 2**: We will first understand Self-Attention (with and without weights).
- **Level 3**: Based on our understanding of Self-Attention, we will explore Causal Attention.
- **Level 4**: Lastly, we will discuss Multi-Head Attention.
- **Bonus**: We will discuss Cross-Attention.

## Medium Article

## Code
